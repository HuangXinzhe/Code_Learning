{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1，我们一般只叶子节点的导数保存，如果想对非叶子节点的导数保存，使用retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "# y.retain_grad()\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "out.backward()\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2,定义神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # 定义神经网络中每层的结构\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    # 构建模型\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##You just have to define the forward function, and the backward function \n",
    "# (where gradients are computed) is automatically defined for you using autograd. \n",
    "# You can use any of the Tensor operations in the forward function.\n",
    "\n",
    "##The learnable parameters of a model are returned by net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001E222C50848>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用模型预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1, 1, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3711, -0.2958,  0.8775,  ...,  0.8098,  0.5758,  0.9222],\n",
       "          [ 0.5273, -0.4217, -1.1892,  ..., -0.5869,  1.9505, -0.2103],\n",
       "          [ 1.6812, -0.7292, -1.2361,  ..., -0.4509, -0.2713,  0.1097],\n",
       "          ...,\n",
       "          [-1.4513, -0.6343, -1.9639,  ...,  2.2364, -0.2389,  0.3598],\n",
       "          [-0.0505,  0.4481, -0.7992,  ...,  0.4705,  2.4979, -1.0948],\n",
       "          [-1.7410,  0.6912, -0.0222,  ...,  0.7940,  0.4846,  0.1252]]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1120,  0.0761, -0.0765, -0.0766,  0.0298,  0.0522, -0.0017,  0.0768,\n",
      "         -0.1018, -0.1242]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = net(input)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### note：torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, \n",
    "#### and not a single sample.\n",
    "#### For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width.\n",
    "#### If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3，定义LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6975, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "#       -> view -> linear -> relu -> linear -> relu -> linear\n",
    "#       -> MSELoss\n",
    "#       -> loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x000001E224424308>\n",
      "<AddmmBackward object at 0x000001E224424AC8>\n",
      "<AccumulateGrad object at 0x000001E224424308>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Indeed. Internally, PyTorch loves to connect all relevant inputs to the graph, \n",
    "# and if those don’t require gradients, you get None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To backpropagate the error all we have to do is to loss.backward(). \n",
    "#### You need to clear the existing gradients though, \n",
    "#### else gradients will be accumulated to existing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0197, -0.0051, -0.0149,  0.0156, -0.0051, -0.0036])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### weight = weight - learning_rate * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 更新梯度\n",
    "# learning_rate = 0.01\n",
    "# for f in net.parameters():\n",
    "#     f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: torch.optim that implements all these methods. Using it is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adadelta(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "# net.zero_grad() \n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe how gradient buffers had to be manually set to zero using optimizer.zero_grad(). This is because gradients are accumulated as explained in Backprop section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 定义类似线性回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "X = boston.data\n",
    "Y = boston.target\n",
    "print(np.shape(X))\n",
    "print(np.shape(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "X = std.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegressionModel(\n",
      "  (linear): Linear(in_features=13, out_features=100, bias=True)\n",
      "  (linear2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## 定义模型\n",
    "class RegressionModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(RegressionModel,self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 100)\n",
    "        self.linear2 = nn.Linear(100, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = f.relu(x)\n",
    "        out = self.linear2(x)\n",
    "        return out\n",
    "# 模型实例化\n",
    "input_dim = 13\n",
    "output_dim = 1\n",
    "model = RegressionModel(input_dim, output_dim)\n",
    "print(model)\n",
    "## 定义损失函数和优化器\n",
    "learning_rate = 0.01\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(torch.nn.MSELoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 585.7290649414062\n",
      "epoch 100, loss 14.637965202331543\n",
      "epoch 200, loss 9.695610046386719\n",
      "epoch 300, loss 7.963910102844238\n",
      "epoch 400, loss 6.784719467163086\n",
      "epoch 500, loss 5.989674091339111\n",
      "epoch 600, loss 5.308857440948486\n",
      "epoch 700, loss 4.740423679351807\n",
      "epoch 800, loss 4.2938127517700195\n",
      "epoch 900, loss 3.983525037765503\n",
      "epoch 1000, loss 3.7086453437805176\n",
      "epoch 1100, loss 3.4333131313323975\n",
      "epoch 1200, loss 3.166347026824951\n",
      "epoch 1300, loss 2.9186582565307617\n",
      "epoch 1400, loss 2.683389902114868\n",
      "epoch 1500, loss 2.464951515197754\n",
      "epoch 1600, loss 2.241133213043213\n",
      "epoch 1700, loss 2.0261950492858887\n",
      "epoch 1800, loss 1.8405084609985352\n",
      "epoch 1900, loss 1.6862565279006958\n",
      "epoch 2000, loss 1.5421223640441895\n",
      "epoch 2100, loss 1.4378657341003418\n",
      "epoch 2200, loss 1.3430838584899902\n",
      "epoch 2300, loss 1.2618528604507446\n",
      "epoch 2400, loss 1.1689767837524414\n",
      "epoch 2500, loss 1.1014906167984009\n",
      "epoch 2600, loss 1.0422548055648804\n",
      "epoch 2700, loss 0.9792011976242065\n",
      "epoch 2800, loss 0.9121278524398804\n",
      "epoch 2900, loss 0.8378015756607056\n",
      "epoch 3000, loss 0.7791131734848022\n",
      "epoch 3100, loss 0.7067148089408875\n",
      "epoch 3200, loss 0.6815796494483948\n",
      "epoch 3300, loss 0.6247186660766602\n",
      "epoch 3400, loss 0.5915573239326477\n",
      "epoch 3500, loss 0.5542039275169373\n",
      "epoch 3600, loss 0.5275799036026001\n",
      "epoch 3700, loss 0.5064544081687927\n",
      "epoch 3800, loss 0.4893263578414917\n",
      "epoch 3900, loss 0.4723931550979614\n",
      "epoch 4000, loss 0.4496738016605377\n",
      "epoch 4100, loss 0.43610861897468567\n",
      "epoch 4200, loss 0.41783103346824646\n",
      "epoch 4300, loss 0.4297170042991638\n",
      "epoch 4400, loss 0.41881999373435974\n",
      "epoch 4500, loss 0.3832387328147888\n",
      "epoch 4600, loss 0.4004628658294678\n",
      "epoch 4700, loss 0.3922181725502014\n",
      "epoch 4800, loss 0.3625926077365875\n",
      "epoch 4900, loss 0.36314618587493896\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#数据转为tensor类型\n",
    "# inputs = torch.as_tensor(torch.from_numpy(X_train),dtype=torch.float32)\n",
    "# labels = torch.as_tensor(torch.from_numpy(Y_train),dtype=torch.float32)\n",
    "inputs = torch.tensor(X_train,dtype=torch.float32)\n",
    "labels = torch.tensor(Y_train,dtype=torch.float32)\n",
    "\n",
    "# inputs = torch.tensor(X_train)\n",
    "# labels = torch.tensor(Y_train)\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    \n",
    "    #前向传播\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    #计算loss\n",
    "    loss = criterion(outputs, labels.view(-1,1))\n",
    "    \n",
    "    #梯度清零\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #反向传播\n",
    "    loss.backward()\n",
    "\n",
    "    #更新参数\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 梯度累计\n",
    "# inputs = torch.tensor(X_train,dtype=torch.float32)\n",
    "# labels = torch.tensor(Y_train,dtype=torch.float32)\n",
    "# model.zero_grad()\n",
    "# epochs = 10000\n",
    "# for epoch in range(epochs):\n",
    "\n",
    "    \n",
    "#     #前向传播\n",
    "#     outputs = model(inputs),在正常情况下，nimi-batch数据需要更新，这里用了全量数据\n",
    "    \n",
    "#     #计算loss\n",
    "#     loss = criterion(outputs, labels.view(-1,1))\n",
    "    \n",
    "\n",
    "\n",
    "#     #反向传播\n",
    "#     loss.backward()\n",
    "\n",
    "    \n",
    "#     if epoch % 100 == 0:\n",
    "#             #更新参数\n",
    "#         optimizer.step()\n",
    "#             #梯度清零\n",
    "#         optimizer.zero_grad()\n",
    "#         print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[21.9926]], grad_fn=<AddmmBackward>)\n",
      "19.9\n"
     ]
    }
   ],
   "source": [
    "input_ = torch.tensor(X_test[5],dtype=torch.float32).view(1,-1)\n",
    "print(model(input_))\n",
    "print(Y_test[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = torch.tensor(X_test,dtype=torch.float32)\n",
    "predicts = model(input_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicts.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8898898827531612"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(predicts.detach().numpy(),Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
