{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造一个5x3矩阵，未初始化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.6837e-39, 4.2246e-39, 1.0286e-38],\n",
      "        [1.0653e-38, 1.0194e-38, 8.4490e-39],\n",
      "        [1.0469e-38, 9.3674e-39, 9.9184e-39],\n",
      "        [8.7245e-39, 9.2755e-39, 8.9082e-39],\n",
      "        [9.9184e-39, 8.4490e-39, 9.6429e-39]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取tensor的大小\n",
    "# torch.Size 实际上是一个元组，所以它支持相同的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function rand:\n",
      "\n",
      "rand(...)\n",
      "    rand(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "    \n",
      "    Returns a tensor filled with random numbers from a uniform distribution\n",
      "    on the interval :math:`[0, 1)`\n",
      "    \n",
      "    The shape of the tensor is defined by the variable argument :attr:`size`.\n",
      "    \n",
      "    Args:\n",
      "        size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "            Can be a variable number of arguments or a collection like a list or tuple.\n",
      "    \n",
      "    Keyword args:\n",
      "        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
      "        out (Tensor, optional): the output tensor.\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
      "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "            Default: ``torch.strided``.\n",
      "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "            Default: if ``None``, uses the current device for the default tensor type\n",
      "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "        requires_grad (bool, optional): If autograd should record operations on the\n",
      "            returned tensor. Default: ``False``.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.rand(4)\n",
      "        tensor([ 0.5204,  0.2503,  0.3525,  0.5673])\n",
      "        >>> torch.rand(2, 3)\n",
      "        tensor([[ 0.8237,  0.5781,  0.6879],\n",
      "                [ 0.3816,  0.7249,  0.0998]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.rand,Returns a tensor filled with random numbers from a uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2585, 0.7857, 0.2294],\n",
       "        [0.2624, 0.1006, 0.2053],\n",
       "        [0.3613, 0.6079, 0.5485],\n",
       "        [0.8599, 0.0896, 0.4978],\n",
       "        [0.0965, 0.9550, 0.2583]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5158, 0.5959, 0.1621],\n",
       "        [0.3573, 0.4468, 0.5675],\n",
       "        [0.5732, 0.3973, 0.1708],\n",
       "        [0.3644, 0.3677, 0.5251],\n",
       "        [0.7759, 0.6641, 0.5169]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7743, 1.3816, 0.3915],\n",
       "        [0.6197, 0.5474, 0.7728],\n",
       "        [0.9346, 1.0052, 0.7192],\n",
       "        [1.2243, 0.4573, 1.0229],\n",
       "        [0.8724, 1.6192, 0.7752]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7743, 1.3816, 0.3915],\n",
      "        [0.6197, 0.5474, 0.7728],\n",
      "        [0.9346, 1.0052, 0.7192],\n",
      "        [1.2243, 0.4573, 1.0229],\n",
      "        [0.8724, 1.6192, 0.7752]])\n"
     ]
    }
   ],
   "source": [
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将Tensor转换为numpy数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7743, 1.3816, 0.3915],\n",
       "        [0.6197, 0.5474, 0.7728],\n",
       "        [0.9346, 1.0052, 0.7192],\n",
       "        [1.2243, 0.4573, 1.0229],\n",
       "        [0.8724, 1.6192, 0.7752]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7743, 2.3816, 1.3915],\n",
       "        [1.6197, 1.5474, 1.7728],\n",
       "        [1.9346, 2.0052, 1.7192],\n",
       "        [2.2243, 1.4573, 2.0229],\n",
       "        [1.8724, 2.6192, 1.7752]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.7742631 2.3816018 1.3914711]\n",
      " [1.6196917 1.5474147 1.7728198]\n",
      " [1.9345767 2.0051804 1.7192353]\n",
      " [2.2242694 1.4573026 2.022867 ]\n",
      " [1.8723872 2.6191833 1.7751737]]\n"
     ]
    }
   ],
   "source": [
    "print(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将numpy数组转换为Torch张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = np.ones(5)\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = torch.from_numpy(a1)\n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ufunc object:\n",
      "\n",
      "add = class ufunc(builtins.object)\n",
      " |  Functions that operate element by element on whole arrays.\n",
      " |  \n",
      " |  To see the documentation for a specific ufunc, use `info`.  For\n",
      " |  example, ``np.info(np.sin)``.  Because ufuncs are written in C\n",
      " |  (for speed) and linked into Python with NumPy's ufunc facility,\n",
      " |  Python's help() function finds this page whenever help() is called\n",
      " |  on a ufunc.\n",
      " |  \n",
      " |  A detailed explanation of ufuncs can be found in the docs for :ref:`ufuncs`.\n",
      " |  \n",
      " |  Calling ufuncs:\n",
      " |  ===============\n",
      " |  \n",
      " |  op(*x[, out], where=True, **kwargs)\n",
      " |  Apply `op` to the arguments `*x` elementwise, broadcasting the arguments.\n",
      " |  \n",
      " |  The broadcasting rules are:\n",
      " |  \n",
      " |  * Dimensions of length 1 may be prepended to either array.\n",
      " |  * Arrays may be repeated along dimensions of length 1.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  *x : array_like\n",
      " |      Input arrays.\n",
      " |  out : ndarray, None, or tuple of ndarray and None, optional\n",
      " |      Alternate array object(s) in which to put the result; if provided, it\n",
      " |      must have a shape that the inputs broadcast to. A tuple of arrays\n",
      " |      (possible only as a keyword argument) must have length equal to the\n",
      " |      number of outputs; use None for uninitialized outputs to be\n",
      " |      allocated by the ufunc.\n",
      " |  where : array_like, optional\n",
      " |      This condition is broadcast over the input. At locations where the\n",
      " |      condition is True, the `out` array will be set to the ufunc result.\n",
      " |      Elsewhere, the `out` array will retain its original value.\n",
      " |      Note that if an uninitialized `out` array is created via the default\n",
      " |      ``out=None``, locations within it where the condition is False will\n",
      " |      remain uninitialized.\n",
      " |  **kwargs\n",
      " |      For other keyword-only arguments, see the :ref:`ufunc docs <ufuncs.kwargs>`.\n",
      " |  \n",
      " |  Returns\n",
      " |  -------\n",
      " |  r : ndarray or tuple of ndarray\n",
      " |      `r` will have the shape that the arrays in `x` broadcast to; if `out` is\n",
      " |      provided, it will be returned. If not, `r` will be allocated and\n",
      " |      may contain uninitialized values. If the function has more than one\n",
      " |      output, then the result will be a tuple of arrays.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, /, *args, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  accumulate(...)\n",
      " |      accumulate(array, axis=0, dtype=None, out=None)\n",
      " |      \n",
      " |      Accumulate the result of applying the operator to all elements.\n",
      " |      \n",
      " |      For a one-dimensional array, accumulate produces results equivalent to::\n",
      " |      \n",
      " |        r = np.empty(len(A))\n",
      " |        t = op.identity        # op = the ufunc being applied to A's  elements\n",
      " |        for i in range(len(A)):\n",
      " |            t = op(t, A[i])\n",
      " |            r[i] = t\n",
      " |        return r\n",
      " |      \n",
      " |      For example, add.accumulate() is equivalent to np.cumsum().\n",
      " |      \n",
      " |      For a multi-dimensional array, accumulate is applied along only one\n",
      " |      axis (axis zero by default; see Examples below) so repeated use is\n",
      " |      necessary if one wants to accumulate over multiple axes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      array : array_like\n",
      " |          The array to act on.\n",
      " |      axis : int, optional\n",
      " |          The axis along which to apply the accumulation; default is zero.\n",
      " |      dtype : data-type code, optional\n",
      " |          The data-type used to represent the intermediate results. Defaults\n",
      " |          to the data-type of the output array if such is provided, or the\n",
      " |          the data-type of the input array if no output array is provided.\n",
      " |      out : ndarray, None, or tuple of ndarray and None, optional\n",
      " |          A location into which the result is stored. If not provided or None,\n",
      " |          a freshly-allocated array is returned. For consistency with\n",
      " |          ``ufunc.__call__``, if given as a keyword, this may be wrapped in a\n",
      " |          1-element tuple.\n",
      " |      \n",
      " |          .. versionchanged:: 1.13.0\n",
      " |             Tuples are allowed for keyword argument.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      r : ndarray\n",
      " |          The accumulated values. If `out` was supplied, `r` is a reference to\n",
      " |          `out`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      1-D array examples:\n",
      " |      \n",
      " |      >>> np.add.accumulate([2, 3, 5])\n",
      " |      array([ 2,  5, 10])\n",
      " |      >>> np.multiply.accumulate([2, 3, 5])\n",
      " |      array([ 2,  6, 30])\n",
      " |      \n",
      " |      2-D array examples:\n",
      " |      \n",
      " |      >>> I = np.eye(2)\n",
      " |      >>> I\n",
      " |      array([[1.,  0.],\n",
      " |             [0.,  1.]])\n",
      " |      \n",
      " |      Accumulate along axis 0 (rows), down columns:\n",
      " |      \n",
      " |      >>> np.add.accumulate(I, 0)\n",
      " |      array([[1.,  0.],\n",
      " |             [1.,  1.]])\n",
      " |      >>> np.add.accumulate(I) # no axis specified = axis zero\n",
      " |      array([[1.,  0.],\n",
      " |             [1.,  1.]])\n",
      " |      \n",
      " |      Accumulate along axis 1 (columns), through rows:\n",
      " |      \n",
      " |      >>> np.add.accumulate(I, 1)\n",
      " |      array([[1.,  1.],\n",
      " |             [0.,  1.]])\n",
      " |  \n",
      " |  at(...)\n",
      " |      at(a, indices, b=None)\n",
      " |      \n",
      " |      Performs unbuffered in place operation on operand 'a' for elements\n",
      " |      specified by 'indices'. For addition ufunc, this method is equivalent to\n",
      " |      ``a[indices] += b``, except that results are accumulated for elements that\n",
      " |      are indexed more than once. For example, ``a[[0,0]] += 1`` will only\n",
      " |      increment the first element once because of buffering, whereas\n",
      " |      ``add.at(a, [0,0], 1)`` will increment the first element twice.\n",
      " |      \n",
      " |      .. versionadded:: 1.8.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      a : array_like\n",
      " |          The array to perform in place operation on.\n",
      " |      indices : array_like or tuple\n",
      " |          Array like index object or slice object for indexing into first\n",
      " |          operand. If first operand has multiple dimensions, indices can be a\n",
      " |          tuple of array like index objects or slice objects.\n",
      " |      b : array_like\n",
      " |          Second operand for ufuncs requiring two operands. Operand must be\n",
      " |          broadcastable over first operand after indexing or slicing.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Set items 0 and 1 to their negative values:\n",
      " |      \n",
      " |      >>> a = np.array([1, 2, 3, 4])\n",
      " |      >>> np.negative.at(a, [0, 1])\n",
      " |      >>> a\n",
      " |      array([-1, -2,  3,  4])\n",
      " |      \n",
      " |      Increment items 0 and 1, and increment item 2 twice:\n",
      " |      \n",
      " |      >>> a = np.array([1, 2, 3, 4])\n",
      " |      >>> np.add.at(a, [0, 1, 2, 2], 1)\n",
      " |      >>> a\n",
      " |      array([2, 3, 5, 4])\n",
      " |      \n",
      " |      Add items 0 and 1 in first array to second array,\n",
      " |      and store results in first array:\n",
      " |      \n",
      " |      >>> a = np.array([1, 2, 3, 4])\n",
      " |      >>> b = np.array([1, 2])\n",
      " |      >>> np.add.at(a, [0, 1], b)\n",
      " |      >>> a\n",
      " |      array([2, 4, 3, 4])\n",
      " |  \n",
      " |  outer(...)\n",
      " |      outer(A, B, **kwargs)\n",
      " |      \n",
      " |      Apply the ufunc `op` to all pairs (a, b) with a in `A` and b in `B`.\n",
      " |      \n",
      " |      Let ``M = A.ndim``, ``N = B.ndim``. Then the result, `C`, of\n",
      " |      ``op.outer(A, B)`` is an array of dimension M + N such that:\n",
      " |      \n",
      " |      .. math:: C[i_0, ..., i_{M-1}, j_0, ..., j_{N-1}] =\n",
      " |         op(A[i_0, ..., i_{M-1}], B[j_0, ..., j_{N-1}])\n",
      " |      \n",
      " |      For `A` and `B` one-dimensional, this is equivalent to::\n",
      " |      \n",
      " |        r = empty(len(A),len(B))\n",
      " |        for i in range(len(A)):\n",
      " |            for j in range(len(B)):\n",
      " |                r[i,j] = op(A[i], B[j]) # op = ufunc in question\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      A : array_like\n",
      " |          First array\n",
      " |      B : array_like\n",
      " |          Second array\n",
      " |      kwargs : any\n",
      " |          Arguments to pass on to the ufunc. Typically `dtype` or `out`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      r : ndarray\n",
      " |          Output array\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.outer : A less powerful version of ``np.multiply.outer``\n",
      " |                    that `ravel`\\ s all inputs to 1D. This exists\n",
      " |                    primarily for compatibility with old code.\n",
      " |      \n",
      " |      tensordot : ``np.tensordot(a, b, axes=((), ()))`` and\n",
      " |                  ``np.multiply.outer(a, b)`` behave same for all\n",
      " |                  dimensions of a and b.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.multiply.outer([1, 2, 3], [4, 5, 6])\n",
      " |      array([[ 4,  5,  6],\n",
      " |             [ 8, 10, 12],\n",
      " |             [12, 15, 18]])\n",
      " |      \n",
      " |      A multi-dimensional example:\n",
      " |      \n",
      " |      >>> A = np.array([[1, 2, 3], [4, 5, 6]])\n",
      " |      >>> A.shape\n",
      " |      (2, 3)\n",
      " |      >>> B = np.array([[1, 2, 3, 4]])\n",
      " |      >>> B.shape\n",
      " |      (1, 4)\n",
      " |      >>> C = np.multiply.outer(A, B)\n",
      " |      >>> C.shape; C\n",
      " |      (2, 3, 1, 4)\n",
      " |      array([[[[ 1,  2,  3,  4]],\n",
      " |              [[ 2,  4,  6,  8]],\n",
      " |              [[ 3,  6,  9, 12]]],\n",
      " |             [[[ 4,  8, 12, 16]],\n",
      " |              [[ 5, 10, 15, 20]],\n",
      " |              [[ 6, 12, 18, 24]]]])\n",
      " |  \n",
      " |  reduce(...)\n",
      " |      reduce(a, axis=0, dtype=None, out=None, keepdims=False, initial=<no value>, where=True)\n",
      " |      \n",
      " |      Reduces `a`'s dimension by one, by applying ufunc along one axis.\n",
      " |      \n",
      " |      Let :math:`a.shape = (N_0, ..., N_i, ..., N_{M-1})`.  Then\n",
      " |      :math:`ufunc.reduce(a, axis=i)[k_0, ..,k_{i-1}, k_{i+1}, .., k_{M-1}]` =\n",
      " |      the result of iterating `j` over :math:`range(N_i)`, cumulatively applying\n",
      " |      ufunc to each :math:`a[k_0, ..,k_{i-1}, j, k_{i+1}, .., k_{M-1}]`.\n",
      " |      For a one-dimensional array, reduce produces results equivalent to:\n",
      " |      ::\n",
      " |      \n",
      " |       r = op.identity # op = ufunc\n",
      " |       for i in range(len(A)):\n",
      " |         r = op(r, A[i])\n",
      " |       return r\n",
      " |      \n",
      " |      For example, add.reduce() is equivalent to sum().\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      a : array_like\n",
      " |          The array to act on.\n",
      " |      axis : None or int or tuple of ints, optional\n",
      " |          Axis or axes along which a reduction is performed.\n",
      " |          The default (`axis` = 0) is perform a reduction over the first\n",
      " |          dimension of the input array. `axis` may be negative, in\n",
      " |          which case it counts from the last to the first axis.\n",
      " |      \n",
      " |          .. versionadded:: 1.7.0\n",
      " |      \n",
      " |          If this is None, a reduction is performed over all the axes.\n",
      " |          If this is a tuple of ints, a reduction is performed on multiple\n",
      " |          axes, instead of a single axis or all the axes as before.\n",
      " |      \n",
      " |          For operations which are either not commutative or not associative,\n",
      " |          doing a reduction over multiple axes is not well-defined. The\n",
      " |          ufuncs do not currently raise an exception in this case, but will\n",
      " |          likely do so in the future.\n",
      " |      dtype : data-type code, optional\n",
      " |          The type used to represent the intermediate results. Defaults\n",
      " |          to the data-type of the output array if this is provided, or\n",
      " |          the data-type of the input array if no output array is provided.\n",
      " |      out : ndarray, None, or tuple of ndarray and None, optional\n",
      " |          A location into which the result is stored. If not provided or None,\n",
      " |          a freshly-allocated array is returned. For consistency with\n",
      " |          ``ufunc.__call__``, if given as a keyword, this may be wrapped in a\n",
      " |          1-element tuple.\n",
      " |      \n",
      " |          .. versionchanged:: 1.13.0\n",
      " |             Tuples are allowed for keyword argument.\n",
      " |      keepdims : bool, optional\n",
      " |          If this is set to True, the axes which are reduced are left\n",
      " |          in the result as dimensions with size one. With this option,\n",
      " |          the result will broadcast correctly against the original `arr`.\n",
      " |      \n",
      " |          .. versionadded:: 1.7.0\n",
      " |      initial : scalar, optional\n",
      " |          The value with which to start the reduction.\n",
      " |          If the ufunc has no identity or the dtype is object, this defaults\n",
      " |          to None - otherwise it defaults to ufunc.identity.\n",
      " |          If ``None`` is given, the first element of the reduction is used,\n",
      " |          and an error is thrown if the reduction is empty.\n",
      " |      \n",
      " |          .. versionadded:: 1.15.0\n",
      " |      \n",
      " |      where : array_like of bool, optional\n",
      " |          A boolean array which is broadcasted to match the dimensions\n",
      " |          of `a`, and selects elements to include in the reduction. Note\n",
      " |          that for ufuncs like ``minimum`` that do not have an identity\n",
      " |          defined, one has to pass in also ``initial``.\n",
      " |      \n",
      " |          .. versionadded:: 1.17.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      r : ndarray\n",
      " |          The reduced array. If `out` was supplied, `r` is a reference to it.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.multiply.reduce([2,3,5])\n",
      " |      30\n",
      " |      \n",
      " |      A multi-dimensional array example:\n",
      " |      \n",
      " |      >>> X = np.arange(8).reshape((2,2,2))\n",
      " |      >>> X\n",
      " |      array([[[0, 1],\n",
      " |              [2, 3]],\n",
      " |             [[4, 5],\n",
      " |              [6, 7]]])\n",
      " |      >>> np.add.reduce(X, 0)\n",
      " |      array([[ 4,  6],\n",
      " |             [ 8, 10]])\n",
      " |      >>> np.add.reduce(X) # confirm: default axis value is 0\n",
      " |      array([[ 4,  6],\n",
      " |             [ 8, 10]])\n",
      " |      >>> np.add.reduce(X, 1)\n",
      " |      array([[ 2,  4],\n",
      " |             [10, 12]])\n",
      " |      >>> np.add.reduce(X, 2)\n",
      " |      array([[ 1,  5],\n",
      " |             [ 9, 13]])\n",
      " |      \n",
      " |      You can use the ``initial`` keyword argument to initialize the reduction\n",
      " |      with a different value, and ``where`` to select specific elements to include:\n",
      " |      \n",
      " |      >>> np.add.reduce([10], initial=5)\n",
      " |      15\n",
      " |      >>> np.add.reduce(np.ones((2, 2, 2)), axis=(0, 2), initial=10)\n",
      " |      array([14., 14.])\n",
      " |      >>> a = np.array([10., np.nan, 10])\n",
      " |      >>> np.add.reduce(a, where=~np.isnan(a))\n",
      " |      20.0\n",
      " |      \n",
      " |      Allows reductions of empty arrays where they would normally fail, i.e.\n",
      " |      for ufuncs without an identity.\n",
      " |      \n",
      " |      >>> np.minimum.reduce([], initial=np.inf)\n",
      " |      inf\n",
      " |      >>> np.minimum.reduce([[1., 2.], [3., 4.]], initial=10., where=[True, False])\n",
      " |      array([ 1., 10.])\n",
      " |      >>> np.minimum.reduce([])\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: zero-size array to reduction operation minimum which has no identity\n",
      " |  \n",
      " |  reduceat(...)\n",
      " |      reduceat(a, indices, axis=0, dtype=None, out=None)\n",
      " |      \n",
      " |      Performs a (local) reduce with specified slices over a single axis.\n",
      " |      \n",
      " |      For i in ``range(len(indices))``, `reduceat` computes\n",
      " |      ``ufunc.reduce(a[indices[i]:indices[i+1]])``, which becomes the i-th\n",
      " |      generalized \"row\" parallel to `axis` in the final result (i.e., in a\n",
      " |      2-D array, for example, if `axis = 0`, it becomes the i-th row, but if\n",
      " |      `axis = 1`, it becomes the i-th column).  There are three exceptions to this:\n",
      " |      \n",
      " |      * when ``i = len(indices) - 1`` (so for the last index),\n",
      " |        ``indices[i+1] = a.shape[axis]``.\n",
      " |      * if ``indices[i] >= indices[i + 1]``, the i-th generalized \"row\" is\n",
      " |        simply ``a[indices[i]]``.\n",
      " |      * if ``indices[i] >= len(a)`` or ``indices[i] < 0``, an error is raised.\n",
      " |      \n",
      " |      The shape of the output depends on the size of `indices`, and may be\n",
      " |      larger than `a` (this happens if ``len(indices) > a.shape[axis]``).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      a : array_like\n",
      " |          The array to act on.\n",
      " |      indices : array_like\n",
      " |          Paired indices, comma separated (not colon), specifying slices to\n",
      " |          reduce.\n",
      " |      axis : int, optional\n",
      " |          The axis along which to apply the reduceat.\n",
      " |      dtype : data-type code, optional\n",
      " |          The type used to represent the intermediate results. Defaults\n",
      " |          to the data type of the output array if this is provided, or\n",
      " |          the data type of the input array if no output array is provided.\n",
      " |      out : ndarray, None, or tuple of ndarray and None, optional\n",
      " |          A location into which the result is stored. If not provided or None,\n",
      " |          a freshly-allocated array is returned. For consistency with\n",
      " |          ``ufunc.__call__``, if given as a keyword, this may be wrapped in a\n",
      " |          1-element tuple.\n",
      " |      \n",
      " |          .. versionchanged:: 1.13.0\n",
      " |             Tuples are allowed for keyword argument.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      r : ndarray\n",
      " |          The reduced values. If `out` was supplied, `r` is a reference to\n",
      " |          `out`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      A descriptive example:\n",
      " |      \n",
      " |      If `a` is 1-D, the function `ufunc.accumulate(a)` is the same as\n",
      " |      ``ufunc.reduceat(a, indices)[::2]`` where `indices` is\n",
      " |      ``range(len(array) - 1)`` with a zero placed\n",
      " |      in every other element:\n",
      " |      ``indices = zeros(2 * len(a) - 1)``, ``indices[1::2] = range(1, len(a))``.\n",
      " |      \n",
      " |      Don't be fooled by this attribute's name: `reduceat(a)` is not\n",
      " |      necessarily smaller than `a`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      To take the running sum of four successive values:\n",
      " |      \n",
      " |      >>> np.add.reduceat(np.arange(8),[0,4, 1,5, 2,6, 3,7])[::2]\n",
      " |      array([ 6, 10, 14, 18])\n",
      " |      \n",
      " |      A 2-D example:\n",
      " |      \n",
      " |      >>> x = np.linspace(0, 15, 16).reshape(4,4)\n",
      " |      >>> x\n",
      " |      array([[ 0.,   1.,   2.,   3.],\n",
      " |             [ 4.,   5.,   6.,   7.],\n",
      " |             [ 8.,   9.,  10.,  11.],\n",
      " |             [12.,  13.,  14.,  15.]])\n",
      " |      \n",
      " |      ::\n",
      " |      \n",
      " |       # reduce such that the result has the following five rows:\n",
      " |       # [row1 + row2 + row3]\n",
      " |       # [row4]\n",
      " |       # [row2]\n",
      " |       # [row3]\n",
      " |       # [row1 + row2 + row3 + row4]\n",
      " |      \n",
      " |      >>> np.add.reduceat(x, [0, 3, 1, 2, 0])\n",
      " |      array([[12.,  15.,  18.,  21.],\n",
      " |             [12.,  13.,  14.,  15.],\n",
      " |             [ 4.,   5.,   6.,   7.],\n",
      " |             [ 8.,   9.,  10.,  11.],\n",
      " |             [24.,  28.,  32.,  36.]])\n",
      " |      \n",
      " |      ::\n",
      " |      \n",
      " |       # reduce such that result has the following two columns:\n",
      " |       # [col1 * col2 * col3, col4]\n",
      " |      \n",
      " |      >>> np.multiply.reduceat(x, [0, 3], 1)\n",
      " |      array([[   0.,     3.],\n",
      " |             [ 120.,     7.],\n",
      " |             [ 720.,    11.],\n",
      " |             [2184.,    15.]])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  identity\n",
      " |      The identity value.\n",
      " |      \n",
      " |      Data attribute containing the identity element for the ufunc, if it has one.\n",
      " |      If it does not, the attribute value is None.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.add.identity\n",
      " |      0\n",
      " |      >>> np.multiply.identity\n",
      " |      1\n",
      " |      >>> np.power.identity\n",
      " |      1\n",
      " |      >>> print(np.exp.identity)\n",
      " |      None\n",
      " |  \n",
      " |  nargs\n",
      " |      The number of arguments.\n",
      " |      \n",
      " |      Data attribute containing the number of arguments the ufunc takes, including\n",
      " |      optional ones.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Typically this value will be one more than what you might expect because all\n",
      " |      ufuncs take  the optional \"out\" argument.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.add.nargs\n",
      " |      3\n",
      " |      >>> np.multiply.nargs\n",
      " |      3\n",
      " |      >>> np.power.nargs\n",
      " |      3\n",
      " |      >>> np.exp.nargs\n",
      " |      2\n",
      " |  \n",
      " |  nin\n",
      " |      The number of inputs.\n",
      " |      \n",
      " |      Data attribute containing the number of arguments the ufunc treats as input.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.add.nin\n",
      " |      2\n",
      " |      >>> np.multiply.nin\n",
      " |      2\n",
      " |      >>> np.power.nin\n",
      " |      2\n",
      " |      >>> np.exp.nin\n",
      " |      1\n",
      " |  \n",
      " |  nout\n",
      " |      The number of outputs.\n",
      " |      \n",
      " |      Data attribute containing the number of arguments the ufunc treats as output.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Since all ufuncs can take output arguments, this will always be (at least) 1.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.add.nout\n",
      " |      1\n",
      " |      >>> np.multiply.nout\n",
      " |      1\n",
      " |      >>> np.power.nout\n",
      " |      1\n",
      " |      >>> np.exp.nout\n",
      " |      1\n",
      " |  \n",
      " |  ntypes\n",
      " |      The number of types.\n",
      " |      \n",
      " |      The number of numerical NumPy types - of which there are 18 total - on which\n",
      " |      the ufunc can operate.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.ufunc.types\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.add.ntypes\n",
      " |      18\n",
      " |      >>> np.multiply.ntypes\n",
      " |      18\n",
      " |      >>> np.power.ntypes\n",
      " |      17\n",
      " |      >>> np.exp.ntypes\n",
      " |      7\n",
      " |      >>> np.remainder.ntypes\n",
      " |      14\n",
      " |  \n",
      " |  signature\n",
      " |      Definition of the core elements a generalized ufunc operates on.\n",
      " |      \n",
      " |      The signature determines how the dimensions of each input/output array\n",
      " |      are split into core and loop dimensions:\n",
      " |      \n",
      " |      1. Each dimension in the signature is matched to a dimension of the\n",
      " |         corresponding passed-in array, starting from the end of the shape tuple.\n",
      " |      2. Core dimensions assigned to the same label in the signature must have\n",
      " |         exactly matching sizes, no broadcasting is performed.\n",
      " |      3. The core dimensions are removed from all inputs and the remaining\n",
      " |         dimensions are broadcast together, defining the loop dimensions.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Generalized ufuncs are used internally in many linalg functions, and in\n",
      " |      the testing suite; the examples below are taken from these.\n",
      " |      For ufuncs that operate on scalars, the signature is None, which is\n",
      " |      equivalent to '()' for every argument.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.core.umath_tests.matrix_multiply.signature\n",
      " |      '(m,n),(n,p)->(m,p)'\n",
      " |      >>> np.linalg._umath_linalg.det.signature\n",
      " |      '(m,m)->()'\n",
      " |      >>> np.add.signature is None\n",
      " |      True  # equivalent to '(),()->()'\n",
      " |  \n",
      " |  types\n",
      " |      Returns a list with types grouped input->output.\n",
      " |      \n",
      " |      Data attribute listing the data-type \"Domain-Range\" groupings the ufunc can\n",
      " |      deliver. The data-types are given using the character codes.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.ufunc.ntypes\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.add.types\n",
      " |      ['??->?', 'bb->b', 'BB->B', 'hh->h', 'HH->H', 'ii->i', 'II->I', 'll->l',\n",
      " |      'LL->L', 'qq->q', 'QQ->Q', 'ff->f', 'dd->d', 'gg->g', 'FF->F', 'DD->D',\n",
      " |      'GG->G', 'OO->O']\n",
      " |      \n",
      " |      >>> np.multiply.types\n",
      " |      ['??->?', 'bb->b', 'BB->B', 'hh->h', 'HH->H', 'ii->i', 'II->I', 'll->l',\n",
      " |      'LL->L', 'qq->q', 'QQ->Q', 'ff->f', 'dd->d', 'gg->g', 'FF->F', 'DD->D',\n",
      " |      'GG->G', 'OO->O']\n",
      " |      \n",
      " |      >>> np.power.types\n",
      " |      ['bb->b', 'BB->B', 'hh->h', 'HH->H', 'ii->i', 'II->I', 'll->l', 'LL->L',\n",
      " |      'qq->q', 'QQ->Q', 'ff->f', 'dd->d', 'gg->g', 'FF->F', 'DD->D', 'GG->G',\n",
      " |      'OO->O']\n",
      " |      \n",
      " |      >>> np.exp.types\n",
      " |      ['f->f', 'd->d', 'g->g', 'F->F', 'D->D', 'G->G', 'O->O']\n",
      " |      \n",
      " |      >>> np.remainder.types\n",
      " |      ['bb->b', 'BB->B', 'hh->h', 'HH->H', 'ii->i', 'II->I', 'll->l', 'LL->L',\n",
      " |      'qq->q', 'QQ->Q', 'ff->f', 'dd->d', 'gg->g', 'OO->O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.add(a1, 1, out=a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> a1为array，b1为由a1转换而来的tensor，两者地址不同但是都是引用同一地址内的数据，所以一个修改另一个同时也修改了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2396476580448\n",
      "2396476581752\n"
     ]
    }
   ],
   "source": [
    "print(id(a1))\n",
    "print(id(b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA传感器\n",
    "# 可以使用该.cuda功能将传感器移动到GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[1,2],[3,4]])\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# 初始化一个全是1的tensor\n",
    "\n",
    "a = torch.ones(2,5)\n",
    "\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# 初始化一个全零的tensor\n",
    "a = torch.zeros(3,3)\n",
    "\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# 初始化一个对角线为1的tensor\n",
    "\n",
    "a = torch.eye(3,3)\n",
    "\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "#定义一个与a形状相同的全零tensor\n",
    "\n",
    "b = torch.zeros_like(a)\n",
    "\n",
    "print(b)\n",
    "print(b.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "#同上\n",
    "b = torch.ones_like(a)\n",
    "\n",
    "print(b)\n",
    "print(b.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function normal:\n",
      "\n",
      "normal(...)\n",
      "    normal(mean, std, *, generator=None, out=None) -> Tensor\n",
      "    \n",
      "    Returns a tensor of random numbers drawn from separate normal distributions\n",
      "    whose mean and standard deviation are given.\n",
      "    \n",
      "    The :attr:`mean` is a tensor with the mean of\n",
      "    each output element's normal distribution\n",
      "    \n",
      "    The :attr:`std` is a tensor with the standard deviation of\n",
      "    each output element's normal distribution\n",
      "    \n",
      "    The shapes of :attr:`mean` and :attr:`std` don't need to match, but the\n",
      "    total number of elements in each tensor need to be the same.\n",
      "    \n",
      "    .. note:: When the shapes do not match, the shape of :attr:`mean`\n",
      "              is used as the shape for the returned output tensor\n",
      "    \n",
      "    .. note:: When :attr:`std` is a CUDA tensor, this function synchronizes\n",
      "              its device with the CPU.\n",
      "    \n",
      "    Args:\n",
      "        mean (Tensor): the tensor of per-element means\n",
      "        std (Tensor): the tensor of per-element standard deviations\n",
      "    \n",
      "    Keyword args:\n",
      "        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\n",
      "        tensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,\n",
      "                  8.0505,   8.1408,   9.0563,  10.0566])\n",
      "    \n",
      "    .. function:: normal(mean=0.0, std, *, out=None) -> Tensor\n",
      "    \n",
      "    Similar to the function above, but the means are shared among all drawn\n",
      "    elements.\n",
      "    \n",
      "    Args:\n",
      "        mean (float, optional): the mean for all distributions\n",
      "        std (Tensor): the tensor of per-element standard deviations\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.normal(mean=0.5, std=torch.arange(1., 6.))\n",
      "        tensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])\n",
      "    \n",
      "    .. function:: normal(mean, std=1.0, *, out=None) -> Tensor\n",
      "    \n",
      "    Similar to the function above, but the standard deviations are shared among\n",
      "    all drawn elements.\n",
      "    \n",
      "    Args:\n",
      "        mean (Tensor): the tensor of per-element means\n",
      "        std (float, optional): the standard deviation for all distributions\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.normal(mean=torch.arange(1., 6.))\n",
      "        tensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])\n",
      "    \n",
      "    .. function:: normal(mean, std, size, *, out=None) -> Tensor\n",
      "    \n",
      "    Similar to the function above, but the means and standard deviations are shared\n",
      "    among all drawn elements. The resulting tensor has size given by :attr:`size`.\n",
      "    \n",
      "    Args:\n",
      "        mean (float): the mean for all distributions\n",
      "        std (float): the standard deviation for all distributions\n",
      "        size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.normal(2, 3, size=(1, 4))\n",
      "        tensor([[-1.3987, -1.9544,  3.6048,  0.7909]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.1904, 14.6012, -1.7961,  3.5772]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "a  = torch.normal(mean=0., std=5.,size=(1, 4))   \n",
    "\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([1, 2])\n",
    "b = torch.Tensor([3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 6.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2.]), tensor([3., 4.]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 6.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.add(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 6.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.add_(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 6.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2.])\n",
      "tensor([4., 6.])\n"
     ]
    }
   ],
   "source": [
    "# 加法运算\n",
    "\n",
    "c = a + b\n",
    "c = torch.add(a, b)\n",
    "c = a.add(b)\n",
    "print(a)\n",
    "c = a.add_(b) \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2.])\n",
      "tensor([4., 6.])\n",
      "tensor([1., 2.])\n",
      "tensor([1., 2.])\n"
     ]
    }
   ],
   "source": [
    "# 减法\n",
    "c = a - b\n",
    "c = torch.sub(a, b)\n",
    "c = a.sub(b)\n",
    "print(c)\n",
    "print(a)\n",
    "c = a.sub_(b) \n",
    "print(c)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 8.])\n",
      "tensor([3., 8.])\n"
     ]
    }
   ],
   "source": [
    "# 乘法运算\n",
    "\n",
    "a = torch.Tensor([1, 2])\n",
    "b = torch.Tensor([3, 4])\n",
    "\n",
    "c = a * b\n",
    "c = torch.mul(a, b)\n",
    "c = a.mul(b)\n",
    "print(c)\n",
    "c = a.mul_(b) \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3333, 0.5000])\n",
      "tensor([0.3333, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "# 除法运算\n",
    "\n",
    "a = torch.Tensor([1, 2])\n",
    "b = torch.Tensor([3, 4])\n",
    "\n",
    "c = a / b\n",
    "c = torch.div(a, b)\n",
    "c = a.div(b)\n",
    "print(c)\n",
    "c = a.div_(b) \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.]]) tensor([[1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵运算 a*b ->(m,n)*(n,k)\n",
    "\n",
    "a = torch.ones(2, 1)\n",
    "b = torch.ones(1, 3)\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# a*b  \n",
    "\n",
    "c = torch.mm(a, b)\n",
    "c = torch.matmul(a, b)\n",
    "print(c)\n",
    "c = a @ b\n",
    "c = a.matmul(b)\n",
    "c = a.mm(b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "# 幂运算\n",
    "\n",
    "c = torch.pow(a, 2)\n",
    "c = a.pow(2)\n",
    "c = a**2\n",
    "c = a.pow_(2)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7183],\n",
      "        [2.7183]])\n"
     ]
    }
   ],
   "source": [
    "# e**n exp\n",
    "\n",
    "c = torch.exp(a)\n",
    "c = a.exp_()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6487],\n",
      "        [1.6487]])\n"
     ]
    }
   ],
   "source": [
    "# 开方运算\n",
    "\n",
    "c = a.sqrt()\n",
    "c = a.sqrt_()\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000]])\n"
     ]
    }
   ],
   "source": [
    "# 对数运算\n",
    "\n",
    "c = torch.log2(a)\n",
    "c = torch.log10(a)\n",
    "c = torch.log(a)\n",
    "c = torch.log_(a)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cat(seq, dim=0, out=None)：按照已经存在的维度进行拼接\n",
    "# torch.stack(seq, dim=0, out=None)：按照新的维度进行拼接\n",
    "#注: .cat 和 .stack的区别在于 cat会增加现有维度的值,可以理解为续接，stack会新加增加一个维度，可以\n",
    "\n",
    "#理解为叠加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((2, 4))\n",
    "b = torch.ones((2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((a, b)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "# torch.cat\n",
    "\n",
    "a = torch.zeros((2, 4))\n",
    "b = torch.ones((2, 4))\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "out = torch.cat((a, b), dim=0) \n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1.]])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "out = torch.cat((a, b), dim=1) \n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 3.6667],\n",
       "        [6.3333, 9.0000]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(1, 9, 4).view(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "tensor([[10., 11., 12.],\n",
      "        [13., 14., 15.],\n",
      "        [16., 17., 18.]])\n"
     ]
    }
   ],
   "source": [
    "#torch.stack\n",
    "\n",
    "a = torch.linspace(1, 9, 9).view(3, 3)\n",
    "b = torch.linspace(10, 18, 9).view(3, 3)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function stack:\n",
      "\n",
      "stack(...)\n",
      "    stack(tensors, dim=0, out=None) -> Tensor\n",
      "    \n",
      "    Concatenates sequence of tensors along a new dimension.\n",
      "    \n",
      "    All tensors need to be of the same size.\n",
      "    \n",
      "    Arguments:\n",
      "        tensors (sequence of Tensors): sequence of tensors to concatenate\n",
      "        dim (int): dimension to insert. Has to be between 0 and the number\n",
      "            of dimensions of concatenated tensors (inclusive)\n",
      "        out (Tensor, optional): the output tensor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.stack((a, b),dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 2])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.chunk(tensor, chunk_num, dim)与torch.cat()原理相反，它是将tensor按dim（行或列）分割成chunk_num个tensor块，返回的是一个元组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.chunk(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1888, 0.9857, 0.3204],\n",
       "         [0.3662, 0.4248, 0.0140]]),\n",
       " tensor([[0.0601, 0.1159, 0.0328]]))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1888, 0.9857, 0.3204],\n",
       "        [0.3662, 0.4248, 0.0140],\n",
       "        [0.0601, 0.1159, 0.0328]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([[0.3204],\n",
      "        [0.0140],\n",
      "        [0.0328]]) torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "out = torch.split(a, 2, dim=1)\n",
    "print(len(out))\n",
    "print(out[1], out[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7087, 0.1431, 0.2755],\n",
      "        [0.5715, 0.3302, 0.7011]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9693, 0.8277, 0.0830, 0.5431, 0.6596, 0.8729])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9693, 0.8277],\n",
      "        [0.0830, 0.5431],\n",
      "        [0.6596, 0.8729]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "out = torch.reshape(a, (3, 2))\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9693, 0.8277, 0.0830],\n",
      "        [0.5431, 0.6596, 0.8729]])\n"
     ]
    }
   ],
   "source": [
    "# view\n",
    "out = a.view(2, 3)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0657, 0.7832, 0.3745, 0.3862, 0.4746],\n",
       "        [0.6102, 0.6137, 0.5335, 0.6711, 0.5053],\n",
       "        [0.8901, 0.7022, 0.2508, 0.1964, 0.6914],\n",
       "        [0.3348, 0.3137, 0.8108, 0.7413, 0.9831]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0657, 0.7832, 0.3745, 0.3862],\n",
       "        [0.4746, 0.6102, 0.6137, 0.5335],\n",
       "        [0.6711, 0.5053, 0.8901, 0.7022],\n",
       "        [0.2508, 0.1964, 0.6914, 0.3348],\n",
       "        [0.3137, 0.8108, 0.7413, 0.9831]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0532, 0.7442],\n",
       "        [0.7235, 0.7221],\n",
       "        [0.4225, 0.5245]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0532, 0.7235, 0.4225],\n",
      "        [0.7442, 0.7221, 0.5245]])\n"
     ]
    }
   ],
   "source": [
    "# torch.transpose 交换2个维度\n",
    "\n",
    "# print(out) #(3, 2)\n",
    "out1 = torch.transpose(a, 0, 1)\n",
    "print(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9586, 0.1505, 0.3778, 0.7283],\n",
       "         [0.4943, 0.3782, 0.0803, 0.5561],\n",
       "         [0.9147, 0.5137, 0.0971, 0.1968]],\n",
       "\n",
       "        [[0.2604, 0.8164, 0.5302, 0.2788],\n",
       "         [0.7760, 0.6747, 0.4669, 0.7376],\n",
       "         [0.3537, 0.7980, 0.1327, 0.1288]]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function transpose:\n",
      "\n",
      "transpose(...)\n",
      "    transpose(input, dim0, dim1) -> Tensor\n",
      "    \n",
      "    Returns a tensor that is a transposed version of :attr:`input`.\n",
      "    The given dimensions :attr:`dim0` and :attr:`dim1` are swapped.\n",
      "    \n",
      "    The resulting :attr:`out` tensor shares it's underlying storage with the\n",
      "    :attr:`input` tensor, so changing the content of one would change the content\n",
      "    of the other.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        dim0 (int): the first dimension to be transposed\n",
      "        dim1 (int): the second dimension to be transposed\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> x = torch.randn(2, 3)\n",
      "        >>> x\n",
      "        tensor([[ 1.0028, -0.9893,  0.5809],\n",
      "                [-0.1669,  0.7299,  0.4942]])\n",
      "        >>> torch.transpose(x, 0, 1)\n",
      "        tensor([[ 1.0028, -0.1669],\n",
      "                [-0.9893,  0.7299],\n",
      "                [ 0.5809,  0.4942]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function permute:\n",
      "\n",
      "permute(...) method of torch.Tensor instance\n",
      "    permute(*dims) -> Tensor\n",
      "    \n",
      "    Returns a view of the original tensor with its dimensions permuted.\n",
      "    \n",
      "    Args:\n",
      "        *dims (int...): The desired ordering of dimensions\n",
      "    \n",
      "    Example:\n",
      "        >>> x = torch.randn(2, 3, 5)\n",
      "        >>> x.size()\n",
      "        torch.Size([2, 3, 5])\n",
      "        >>> x.permute(2, 0, 1).size()\n",
      "        torch.Size([5, 2, 3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(y.permute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.permute(2,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function full:\n",
      "\n",
      "full(...)\n",
      "    full(size, fill_value, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "    \n",
      "    Creates a tensor of size :attr:`size` filled with :attr:`fill_value`. The\n",
      "    tensor's dtype is inferred from :attr:`fill_value`.\n",
      "    \n",
      "    Args:\n",
      "        size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n",
      "            shape of the output tensor.\n",
      "        fill_value (Scalar): the value to fill the output tensor with.\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
      "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "            Default: ``torch.strided``.\n",
      "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "            Default: if ``None``, uses the current device for the default tensor type\n",
      "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "        requires_grad (bool, optional): If autograd should record operations on the\n",
      "            returned tensor. Default: ``False``.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.full((2, 3), 3.141592)\n",
      "        tensor([[ 3.1416,  3.1416,  3.1416],\n",
      "                [ 3.1416,  3.1416,  3.1416]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20., 20., 20., 20., 20.],\n",
      "        [20., 20., 20., 20., 20.],\n",
      "        [20., 20., 20., 20., 20.],\n",
      "        [20., 20., 20., 20., 20.]])\n"
     ]
    }
   ],
   "source": [
    "out = torch.full((4, 5), 20.)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
