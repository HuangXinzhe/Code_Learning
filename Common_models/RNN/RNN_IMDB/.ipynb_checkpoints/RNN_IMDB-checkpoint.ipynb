{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个情感分类的例子，输入是电影评论，输出是；label：0 或1。0代表消极评论；1代表积极评论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/1.png' >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchtext:用来处理文本的一个包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1，读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e9e841d5b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#data：用来生成field\n",
    "from torchtext.legacy import data\n",
    "\n",
    "SEED = 455\n",
    "\n",
    "#在需要生成随机数的实验中，确保每次运行.py文件时，\n",
    "#生成的随机数都是固定的，这样每次实验结果显示也就一致了。\n",
    "#设定random seeds使实验可以复现。\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(1)\n",
    "# torch.rand(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True   # 固定cuda的seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field 包含一些文本处理的通用参数的设置，同时还包含一个词典对象，可以把文本数据表示成数字类型，\n",
    "# 进而可以把文本表示成需要的tensor类型\n",
    "# tokenize：The function used to tokenize strings using this field into\n",
    "#           sequential examples. \n",
    "    \n",
    "# tokenize='spacy'，这表示我们会用spaCy tokenizer来tokenize英文句子。\n",
    "# 如果我们不特别声明tokenize这个参数，那么默认的分词方法是使用空格。\n",
    "\n",
    "# tokenizer_language:tells torchtext which spaCy model to use.\n",
    "\n",
    "# Field :主要包含以下数据预处理的配置信息，比如指定分词方法，是否转成小写，\n",
    "#     起始字符，结束字符，补全字符以及词典等等    \n",
    "    \n",
    "TEXT = data.Field(tokenize = 'spacy',\n",
    "                 tokenizer_language = 'en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Field in module torchtext.legacy.data.field:\n",
      "\n",
      "class Field(RawField)\n",
      " |  Field(sequential=True, use_vocab=True, init_token=None, eos_token=None, fix_length=None, dtype=torch.int64, preprocessing=None, postprocessing=None, lower=False, tokenize=None, tokenizer_language='en', include_lengths=False, batch_first=False, pad_token='<pad>', unk_token='<unk>', pad_first=False, truncate_first=False, stop_words=None, is_target=False)\n",
      " |  \n",
      " |  Defines a datatype together with instructions for converting to Tensor.\n",
      " |  \n",
      " |  Field class models common text processing datatypes that can be represented\n",
      " |  by tensors.  It holds a Vocab object that defines the set of possible values\n",
      " |  for elements of the field and their corresponding numerical representations.\n",
      " |  The Field object also holds other parameters relating to how a datatype\n",
      " |  should be numericalized, such as a tokenization method and the kind of\n",
      " |  Tensor that should be produced.\n",
      " |  \n",
      " |  If a Field is shared between two columns in a dataset (e.g., question and\n",
      " |  answer in a QA dataset), then they will have a shared vocabulary.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      sequential: Whether the datatype represents sequential data. If False,\n",
      " |          no tokenization is applied. Default: True.\n",
      " |      use_vocab: Whether to use a Vocab object. If False, the data in this\n",
      " |          field should already be numerical. Default: True.\n",
      " |      init_token: A token that will be prepended to every example using this\n",
      " |          field, or None for no initial token. Default: None.\n",
      " |      eos_token: A token that will be appended to every example using this\n",
      " |          field, or None for no end-of-sentence token. Default: None.\n",
      " |      fix_length: A fixed length that all examples using this field will be\n",
      " |          padded to, or None for flexible sequence lengths. Default: None.\n",
      " |      dtype: The torch.dtype class that represents a batch of examples\n",
      " |          of this kind of data. Default: torch.long.\n",
      " |      preprocessing: The Pipeline that will be applied to examples\n",
      " |          using this field after tokenizing but before numericalizing. Many\n",
      " |          Datasets replace this attribute with a custom preprocessor.\n",
      " |          Default: None.\n",
      " |      postprocessing: A Pipeline that will be applied to examples using\n",
      " |          this field after numericalizing but before the numbers are turned\n",
      " |          into a Tensor. The pipeline function takes the batch as a list, and\n",
      " |          the field's Vocab.\n",
      " |          Default: None.\n",
      " |      lower: Whether to lowercase the text in this field. Default: False.\n",
      " |      tokenize: The function used to tokenize strings using this field into\n",
      " |          sequential examples. If \"spacy\", the SpaCy tokenizer is\n",
      " |          used. If a non-serializable function is passed as an argument,\n",
      " |          the field will not be able to be serialized. Default: string.split.\n",
      " |      tokenizer_language: The language of the tokenizer to be constructed.\n",
      " |          Various languages currently supported only in SpaCy.\n",
      " |      include_lengths: Whether to return a tuple of a padded minibatch and\n",
      " |          a list containing the lengths of each examples, or just a padded\n",
      " |          minibatch. Default: False.\n",
      " |      batch_first: Whether to produce tensors with the batch dimension first.\n",
      " |          Default: False.\n",
      " |      pad_token: The string token used as padding. Default: \"<pad>\".\n",
      " |      unk_token: The string token used to represent OOV words. Default: \"<unk>\".\n",
      " |      pad_first: Do the padding of the sequence at the beginning. Default: False.\n",
      " |      truncate_first: Do the truncating of the sequence at the beginning. Default: False\n",
      " |      stop_words: Tokens to discard during the preprocessing step. Default: None\n",
      " |      is_target: Whether this field is a target variable.\n",
      " |          Affects iteration over batches. Default: False\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Field\n",
      " |      RawField\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __init__(self, sequential=True, use_vocab=True, init_token=None, eos_token=None, fix_length=None, dtype=torch.int64, preprocessing=None, postprocessing=None, lower=False, tokenize=None, tokenizer_language='en', include_lengths=False, batch_first=False, pad_token='<pad>', unk_token='<unk>', pad_first=False, truncate_first=False, stop_words=None, is_target=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  build_vocab(self, *args, **kwargs)\n",
      " |      Construct the Vocab object for this field from one or more datasets.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          Positional arguments: Dataset objects or other iterable data\n",
      " |              sources from which to construct the Vocab object that\n",
      " |              represents the set of possible values for this field. If\n",
      " |              a Dataset object is provided, all columns corresponding\n",
      " |              to this field are used; individual columns can also be\n",
      " |              provided directly.\n",
      " |          Remaining keyword arguments: Passed to the constructor of Vocab.\n",
      " |  \n",
      " |  numericalize(self, arr, device=None)\n",
      " |      Turn a batch of examples that use this field into a Variable.\n",
      " |      \n",
      " |      If the field has include_lengths=True, a tensor of lengths will be\n",
      " |      included in the return value.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          arr (List[List[str]], or tuple of (List[List[str]], List[int])): List of tokenized\n",
      " |              and padded examples, or tuple of List of\n",
      " |              tokenized and padded examples and List of lengths of each\n",
      " |              example if self.include_lengths is True.\n",
      " |          device (str or torch.device): A string or instance of `torch.device`\n",
      " |              specifying which device the Variables are going to be created on.\n",
      " |              If left as default, the tensors will be created on cpu. Default: None.\n",
      " |  \n",
      " |  pad(self, minibatch)\n",
      " |      Pad a batch of examples using this field.\n",
      " |      \n",
      " |      Pads to self.fix_length if provided, otherwise pads to the length of\n",
      " |      the longest example in the batch. Prepends self.init_token and appends\n",
      " |      self.eos_token if those attributes are not None. Returns a tuple of the\n",
      " |      padded list and a list containing lengths of each example if\n",
      " |      `self.include_lengths` is `True` and `self.sequential` is `True`, else just\n",
      " |      returns the padded list. If `self.sequential` is `False`, no padding is applied.\n",
      " |  \n",
      " |  preprocess(self, x)\n",
      " |      Load a single example using this field, tokenizing if necessary.\n",
      " |      \n",
      " |      If `sequential=True`, the input will be tokenized. Then the input\n",
      " |      will be optionally lowercased and passed to the user-provided\n",
      " |      `preprocessing` Pipeline.\n",
      " |  \n",
      " |  process(self, batch, device=None)\n",
      " |      Process a list of examples to create a torch.Tensor.\n",
      " |      \n",
      " |      Pad, numericalize, and postprocess a batch and create a tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch (list(object)): A list of object from a batch of examples.\n",
      " |      Returns:\n",
      " |          torch.autograd.Variable: Processed object given the input\n",
      " |          and custom postprocessing Pipeline.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  dtypes = {torch.float32: <class 'float'>, torch.float64: <class 'float...\n",
      " |  \n",
      " |  ignore = ['dtype', 'tokenize']\n",
      " |  \n",
      " |  vocab_cls = <class 'torchtext.legacy.vocab.Vocab'>\n",
      " |      Defines a vocabulary object that will be used to numericalize a field.\n",
      " |      \n",
      " |      Attributes:\n",
      " |          freqs: A collections.Counter object holding the frequencies of tokens\n",
      " |              in the data used to build the Vocab.\n",
      " |          stoi: A collections.defaultdict instance mapping token strings to\n",
      " |              numerical identifiers.\n",
      " |          itos: A list of token strings indexed by their numerical identifiers.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from RawField:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(data.Field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = data.LabelField(dtype = torch.float)   # 必须指明float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 84.1M/84.1M [00:26<00:00, 3.19MB/s]\n"
     ]
    }
   ],
   "source": [
    "# 生成数据集\n",
    "from torchtext.legacy import datasets\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL,root='.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method splits in module torchtext.legacy.datasets.imdb:\n",
      "\n",
      "splits(text_field, label_field, root='.data', train='train', test='test', **kwargs) method of builtins.type instance\n",
      "    Create dataset objects for splits of the IMDB dataset.\n",
      "    \n",
      "    Args:\n",
      "        text_field: The field that will be used for the sentence.\n",
      "        label_field: The field that will be used for label data.\n",
      "        root: Root dataset storage directory. Default is '.data'.\n",
      "        train: The directory that contains the training examples\n",
      "        test: The directory that contains the test examples\n",
      "        Remaining keyword arguments: Passed to the splits method of\n",
      "            Dataset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datasets.IMDB.splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The IMDb dataset consists of 50,000 movie reviews, \n",
    "# each marked as being a positive or negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.legacy.data.example.Example at 0x1e9d1df12c8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'Teachers', '\"', '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'Teachers', '\"', '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "# vars() 函数返回对象object的属性和属性值的字典对象。\n",
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method split in module torchtext.legacy.data.dataset:\n",
      "\n",
      "split(split_ratio=0.7, stratified=False, strata_field='label', random_state=None) method of torchtext.legacy.datasets.imdb.IMDB instance\n",
      "    Create train-test(-valid?) splits from the instance's examples.\n",
      "    \n",
      "    Arguments:\n",
      "        split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "            of data to be used for the training split (rest is used for test),\n",
      "            or a list of numbers denoting the relative sizes of train, test and valid\n",
      "            splits respectively. If the relative size for valid is missing, only the\n",
      "            train-test split is returned. Default is 0.7 (for the train set).\n",
      "        stratified (bool): whether the sampling should be stratified.\n",
      "            Default is False.\n",
      "        strata_field (str): name of the examples Field stratified over.\n",
      "            Default is 'label' for the conventional label field.\n",
      "        random_state (tuple): the random seed used for shuffling.\n",
      "            A return value of `random.getstate()`.\n",
      "    \n",
      "    Returns:\n",
      "        Tuple[Dataset]: Datasets for train, validation, and\n",
      "        test splits in that order, if the splits are provided.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_data.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2，建立词典\n",
    "<img src=\"imgs/2.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我们建立一个25000的词典，不在词典的用<unk>表示\n",
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 这里面有2个特殊字符\\<unk>和\\<pad>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stoi:s to i\n",
    "TEXT.vocab.stoi['<unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于一个batch里的数据，不够长度的用<pad>填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method build_vocab in module torchtext.legacy.data.field:\n",
      "\n",
      "build_vocab(*args, **kwargs) method of torchtext.legacy.data.field.Field instance\n",
      "    Construct the Vocab object for this field from one or more datasets.\n",
      "    \n",
      "    Arguments:\n",
      "        Positional arguments: Dataset objects or other iterable data\n",
      "            sources from which to construct the Vocab object that\n",
      "            represents the set of possible values for this field. If\n",
      "            a Dataset object is provided, all columns corresponding\n",
      "            to this field are used; individual columns can also be\n",
      "            provided directly.\n",
      "        Remaining keyword arguments: Passed to the constructor of Vocab.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TEXT.build_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 202979), (',', 192741), ('.', 165709), ('a', 109773), ('and', 109607), ('of', 100946), ('to', 93686), ('is', 76282), ('in', 61365), ('I', 53604), ('it', 53386), ('that', 49188), ('\"', 43962), (\"'s\", 43580), ('this', 42171), ('-', 37189), ('/><br', 35640), ('was', 34659), ('as', 30637), ('with', 30064)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'pos': 0, 'neg': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把数据转换成迭代器的形式，data.BucketIterator.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取一个例子查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_batch_train_examples = next(iter(train_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sizet为[sen len, batch size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   25,    66,  1216, 12879,  3288,    55, 14068,   607,   643,  1448,\n",
       "           11,   149,   149,   787,    11,   112,   607,  3665,  1379,  5386,\n",
       "        17210,    11,  9086,    66,  2537,    25,    11,    11,  1512,   370,\n",
       "          149,   156,   544,    66,   149,    14,    66,  1433,   462,  4182,\n",
       "           11,   908,   806,    11,    66,  3146,   806,  8654,   462,   272,\n",
       "           14,   402,    11,     2,   149,   170,    66,    11,  6716,  1552,\n",
       "           66,   472,    11,  1267], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_batch_train_examples.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_batch_train_examples.label[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3，建立模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1,Each batch, text is a tensor of size [sentence length, batch size]\n",
    "#### 2,embedded is a tensor of size [sentence length, batch size, embedding dim].\n",
    "#### 3, output of size [sentence length, batch size, hidden dim] \n",
    "#### 4,hidden of size [1, batch size, hidden dim]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0,2,0.3,0.5]\n",
    "# [0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sen len, batch size]\n",
    "        # 在这里，我们不需要指明one-hot的维度，pytorch会自动把索引转换为one-hot向量。\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The embedding dimension is the size of the dense word vectors. \n",
    "# This is usually around 50-250 dimensions, but depends on the size of the vocabulary.\n",
    "\n",
    "# The hidden dimension is the size of the hidden states. \n",
    "# This is usually around 100-500 dimensions, \n",
    "# but also depends on factors such as on the vocabulary size, \n",
    "# the size of the dense vectors and the complexity of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,592,105 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4,Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()#包含sigmoid和交叉熵损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义正确率函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.round(torch.tensor(0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train() is used to put the model in \"training mode\", \n",
    "# which turns on dropout and batch normalization. \n",
    "# Although we aren't using them in this model, \n",
    "# it's good practice to include it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(batch.text).squeeze(1)#size: [batch_size,1]\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval() puts the model in \"evaluation mode\", \n",
    "# this turns off dropout and batch normalization. Again, \n",
    "# we are not using them in this model, but it is good practice to include them.\n",
    "\n",
    "# No gradients are calculated on PyTorch operations inside the with no_grad() block. \n",
    "# This causes less memory to be used and speeds up computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()   # 关闭dropout，正则化\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.694 | Train Acc: 49.88%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 50.55%\n",
      "Epoch: 02 | Epoch Time: 0m 31s\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.99%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 49.67%\n",
      "Epoch: 03 | Epoch Time: 0m 31s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.35%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 49.32%\n",
      "Epoch: 04 | Epoch Time: 0m 33s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.21%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 49.47%\n",
      "Epoch: 05 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.24%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 49.54%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "#     else:\n",
    "#         break\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
